开始处理所有模型...
正在处理模型: qwen2_vl_mm_only-7b_3072_bsz128_1e3_pretrain_llava_stage2_1M_job_7660
INFO 01-17 16:52:07 config.py:350] This model supports multiple tasks: {'generate', 'embedding'}. Defaulting to 'generate'.
INFO 01-17 16:52:07 llm_engine.py:249] Initializing an LLM engine (v0.6.4) with config: model='/mnt/lingjiejiang/multimodal_code/exp/saves/qwen2_vl_mm_only-7b_3072_bsz128_1e3_pretrain_llava_stage2_1M_job/sft/full/checkpoint-7660', speculative_config=None, tokenizer='/mnt/lingjiejiang/multimodal_code/exp/saves/qwen2_vl_mm_only-7b_3072_bsz128_1e3_pretrain_llava_stage2_1M_job/sft/full/checkpoint-7660', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/mnt/lingjiejiang/multimodal_code/exp/saves/qwen2_vl_mm_only-7b_3072_bsz128_1e3_pretrain_llava_stage2_1M_job/sft/full/checkpoint-7660, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)
INFO 01-17 16:52:07 selector.py:135] Using Flash Attention backend.
INFO 01-17 16:52:08 model_runner.py:1072] Starting to load model /mnt/lingjiejiang/multimodal_code/exp/saves/qwen2_vl_mm_only-7b_3072_bsz128_1e3_pretrain_llava_stage2_1M_job/sft/full/checkpoint-7660...
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:26,  8.89s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:33<00:36, 18.34s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [02:24<01:00, 60.39s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [02:27<00:00, 37.64s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [02:27<00:00, 36.76s/it]

INFO 01-17 16:54:35 model_runner.py:1077] Loading model weights took 15.5083 GB
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
INFO 01-17 16:54:40 worker.py:232] Memory profiling results: total_gpu_memory=79.25GiB initial_memory_usage=16.15GiB peak_torch_memory=20.53GiB memory_usage_post_profile=16.55Gib non_torch_memory=1.04GiB kv_cache_size=49.77GiB gpu_memory_utilization=0.90
INFO 01-17 16:54:40 gpu_executor.py:113] # GPU blocks: 58239, # CPU blocks: 4681
INFO 01-17 16:54:40 gpu_executor.py:117] Maximum concurrency for 32768 tokens per request: 28.44x
INFO 01-17 16:54:45 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-17 16:54:45 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-17 16:54:59 model_runner.py:1518] Graph capturing finished in 14 secs, took 0.37 GiB
Processing 500 files
INFO 01-17 16:55:01 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114
INFO 01-17 16:55:02 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114
INFO 01-17 16:55:02 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114
INFO 01-17 16:55:02 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114
INFO 01-17 16:55:02 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114
INFO 01-17 16:55:02 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114
INFO 01-17 16:55:02 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114
INFO 01-17 16:55:02 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114
INFO 01-17 16:55:02 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114
INFO 01-17 16:55:02 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114
INFO 01-17 16:55:02 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114
INFO 01-17 16:55:02 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114
INFO 01-17 16:55:02 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114
INFO 01-17 16:55:02 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114
INFO 01-17 16:55:02 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114
INFO 01-17 16:55:02 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114
INFO 01-17 16:55:02 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114
INFO 01-17 16:55:02 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114
INFO 01-17 16:55:02 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114
INFO 01-17 16:55:02 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114
INFO 01-17 16:55:02 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114
INFO 01-17 16:55:02 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114
INFO 01-17 16:55:02 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114
INFO 01-17 16:55:02 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114
INFO 01-17 16:55:02 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114
INFO 01-17 16:55:02 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114
INFO 01-17 16:55:02 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114
INFO 01-17 16:55:02 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114
INFO 01-17 16:55:02 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114
INFO 01-17 16:55:02 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114
INFO 01-17 16:55:02 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114
INFO 01-17 16:55:02 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:04<02:33,  4.94s/it, est. speed input: 152.62 toks/s, output: 6.07 toks/s]Processed prompts:   6%|▋         | 2/32 [00:05<01:07,  2.24s/it, est. speed input: 399.96 toks/s, output: 15.29 toks/s]Processed prompts:   9%|▉         | 3/32 [00:05<00:37,  1.30s/it, est. speed input: 565.62 toks/s, output: 25.97 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:05<00:16,  1.64it/s, est. speed input: 863.27 toks/s, output: 49.32 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:05<00:10,  2.47it/s, est. speed input: 1087.44 toks/s, output: 74.14 toks/s]Processed prompts:  31%|███▏      | 10/32 [00:06<00:05,  3.90it/s, est. speed input: 1470.35 toks/s, output: 117.60 toks/s]Processed prompts:  34%|███▍      | 11/32 [00:06<00:05,  4.15it/s, est. speed input: 1534.72 toks/s, output: 132.96 toks/s]Processed prompts:  41%|████      | 13/32 [00:06<00:03,  4.85it/s, est. speed input: 1747.96 toks/s, output: 166.02 toks/s]Processed prompts:  50%|█████     | 16/32 [00:06<00:02,  6.64it/s, est. speed input: 2003.77 toks/s, output: 222.70 toks/s]Processed prompts:  56%|█████▋    | 18/32 [00:07<00:02,  6.85it/s, est. speed input: 2102.36 toks/s, output: 259.91 toks/s]Processed prompts:  59%|█████▉    | 19/32 [00:07<00:01,  6.95it/s, est. speed input: 2166.19 toks/s, output: 279.59 toks/s]Processed prompts:  72%|███████▏  | 23/32 [00:07<00:00, 11.23it/s, est. speed input: 2598.38 toks/s, output: 373.51 toks/s]Processed prompts:  78%|███████▊  | 25/32 [00:07<00:00, 10.71it/s, est. speed input: 2694.66 toks/s, output: 414.95 toks/s]Processed prompts:  84%|████████▍ | 27/32 [00:08<00:01,  4.89it/s, est. speed input: 2591.61 toks/s, output: 425.67 toks/s]Processed prompts:  88%|████████▊ | 28/32 [00:09<00:01,  3.92it/s, est. speed input: 2551.85 toks/s, output: 435.89 toks/s]Processed prompts:  91%|█████████ | 29/32 [00:09<00:00,  3.80it/s, est. speed input: 2599.46 toks/s, output: 457.67 toks/s]Processed prompts:  94%|█████████▍| 30/32 [00:13<00:02,  1.02s/it, est. speed input: 1947.57 toks/s, output: 373.77 toks/s]Processed prompts:  97%|█████████▋| 31/32 [00:58<00:11, 11.24s/it, est. speed input: 462.87 toks/s, output: 155.75 toks/s] Processed prompts: 100%|██████████| 32/32 [00:58<00:00,  1.82s/it, est. speed input: 475.81 toks/s, output: 226.04 toks/s]
INFO 01-17 16:56:01 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114
INFO 01-17 16:56:01 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114
INFO 01-17 16:56:01 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114
INFO 01-17 16:56:01 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114
INFO 01-17 16:56:01 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114
INFO 01-17 16:56:01 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114
INFO 01-17 16:56:01 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114
INFO 01-17 16:56:01 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114
INFO 01-17 16:56:01 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114
INFO 01-17 16:56:01 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114
INFO 01-17 16:56:01 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114
INFO 01-17 16:56:01 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114
INFO 01-17 16:56:01 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114
INFO 01-17 16:56:01 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114
INFO 01-17 16:56:01 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114
INFO 01-17 16:56:01 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114
INFO 01-17 16:56:01 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114
INFO 01-17 16:56:01 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114
INFO 01-17 16:56:01 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114
INFO 01-17 16:56:01 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114
INFO 01-17 16:56:01 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114
INFO 01-17 16:56:01 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114
INFO 01-17 16:56:01 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114
INFO 01-17 16:56:01 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114
INFO 01-17 16:56:01 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114
INFO 01-17 16:56:01 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114
INFO 01-17 16:56:01 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114
INFO 01-17 16:56:01 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114
INFO 01-17 16:56:01 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114
INFO 01-17 16:56:01 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114
INFO 01-17 16:56:01 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114
INFO 01-17 16:56:01 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:05<02:40,  5.19s/it, est. speed input: 194.00 toks/s, output: 4.62 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:05<00:28,  1.03s/it, est. speed input: 761.60 toks/s, output: 20.70 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:05<00:16,  1.59it/s, est. speed input: 1001.42 toks/s, output: 35.68 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:05<00:13,  1.90it/s, est. speed input: 1158.89 toks/s, output: 44.65 toks/s]Processed prompts:  28%|██▊       | 9/32 [00:05<00:07,  2.89it/s, est. speed input: 1406.85 toks/s, output: 65.81 toks/s]Processed prompts:  31%|███▏      | 10/32 [00:06<00:06,  3.21it/s, est. speed input: 1571.79 toks/s, output: 76.97 toks/s]Processed prompts:  34%|███▍      | 11/32 [00:06<00:06,  3.20it/s, est. speed input: 1630.81 toks/s, output: 88.89 toks/s]Processed prompts:  41%|████      | 13/32 [00:06<00:04,  4.09it/s, est. speed input: 1771.64 toks/s, output: 117.94 toks/s]Processed prompts:  50%|█████     | 16/32 [00:06<00:02,  6.27it/s, est. speed input: 2107.00 toks/s, output: 168.91 toks/s]Processed prompts:  53%|█████▎    | 17/32 [00:07<00:02,  6.70it/s, est. speed input: 2181.73 toks/s, output: 186.17 toks/s]Processed prompts:  56%|█████▋    | 18/32 [00:07<00:02,  5.86it/s, est. speed input: 2244.32 toks/s, output: 201.13 toks/s]Processed prompts:  59%|█████▉    | 19/32 [00:07<00:02,  5.49it/s, est. speed input: 2312.40 toks/s, output: 217.98 toks/s]Processed prompts:  62%|██████▎   | 20/32 [00:07<00:02,  5.53it/s, est. speed input: 2386.06 toks/s, output: 236.80 toks/s]Processed prompts:  66%|██████▌   | 21/32 [00:07<00:01,  5.69it/s, est. speed input: 2461.36 toks/s, output: 256.68 toks/s]Processed prompts:  69%|██████▉   | 22/32 [00:08<00:01,  5.84it/s, est. speed input: 2559.13 toks/s, output: 277.18 toks/s]Processed prompts:  72%|███████▏  | 23/32 [00:08<00:01,  6.44it/s, est. speed input: 2616.03 toks/s, output: 299.53 toks/s]Processed prompts:  75%|███████▌  | 24/32 [00:08<00:02,  3.23it/s, est. speed input: 2499.64 toks/s, output: 305.57 toks/s]Processed prompts:  78%|███████▊  | 25/32 [00:09<00:02,  3.20it/s, est. speed input: 2518.72 toks/s, output: 326.08 toks/s]Processed prompts:  84%|████████▍ | 27/32 [00:09<00:00,  5.10it/s, est. speed input: 2649.40 toks/s, output: 385.35 toks/s]Processed prompts:  88%|████████▊ | 28/32 [00:09<00:00,  4.26it/s, est. speed input: 2656.89 toks/s, output: 404.41 toks/s]Processed prompts:  94%|█████████▍| 30/32 [00:11<00:01,  1.91it/s, est. speed input: 2373.72 toks/s, output: 407.00 toks/s]