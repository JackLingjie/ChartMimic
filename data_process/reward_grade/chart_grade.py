import json  
import os  
from concurrent.futures import ThreadPoolExecutor, as_completed  
from tqdm import tqdm  
from gpt4o import Openai, API_INFOS  
import re  
import base64
# Define the prompt template  
PROMPT_TEMPLATE = "You are an excellent judge at evaluating visualization chart plots. The first image (reference image) is created using ground truth matplotlib code, and the second image (AI-generated image) is created using matplotlib code generated by an AI assistant. Your task is to score how well the AI-generated plot matches the ground truth plot.\n\n### Scoring Methodology:\nThe AI-generated image's score is based on the following criteria, totaling a score out of 100 points:\n\n1. **Chart Types (20 points)** Does the AI-generated image include all chart types present in the reference image (e.g., line charts, bar charts, etc.)?\n2. **Layout (10 points)** Does the arrangement of subplots in the AI-generated image match the reference image (e.g., number of rows and columns)?\n3. **Text Content (20 points)** Does the AI-generated image include all text from the reference image (e.g., titles, annotations, axis labels), excluding axis tick labels?\n4. **Data (20 points)** How accurately do the data trends in the AI-generated image resemble those in the original image and is the number of data groups the same as in the reference image?\n5. **Style (20 points)** Does the AI-generated image match the original in terms of colors (line colors, fill colors, etc.), marker types (point shapes, line styles, etc.), legends, grids, and other stylistic details?\n6. **Clarity (10 points)** Is the AI-generated image clear and free of overlapping elements?\n\n### Evaluation:\nCompare the two images head to head and provide a detailed assessment. Use the following format for your response:\n\n\n---\n\nComments:\n- Chart Types: ${your comment and subscore}\n- Layout: ${your comment and subscore}\n- Text Content: ${your comment and subscore}\n- Data: ${your comment and subscore}\n- Style: ${your comment and subscore}\n- Clarity: ${your comment and subscore}\n\nScore: ${your final score out of 100}\n\n---\n\nPlease use the above format to ensure the evaluation is clear and comprehensive.\n"

  
MACHINE_ID = 1

def load_data(file_path):
    try:
        with open(file_path, 'r') as f:
            return json.load(f)
    except Exception as e:
        print(f"Error loading data from {file_path}: {e}")
        return []

def construct_conversation(original_png, generated_png):
    def encode_image(image_path):
        with open(image_path, "rb") as image_file:
            return base64.b64encode(image_file.read()).decode('utf-8')   
    base64_image1 = encode_image(original_png)      
    base64_image2 = encode_image(generated_png)
    system = "You are a helpful assistant."
    messages = [
        {"role": "system", "content": system},
        {"role": "user", "content": [
            {"type": "text", "text": f"{PROMPT_TEMPLATE}\n"},
            {
                "image":base64_image1
            },
            {
                "image":base64_image2
            }
        ]
            },
    ]
    return messages

def get_revised_text(client, prompt, max_tokens=2048):
    try:
        gpt_answer, stop_reason = client.get_image_response_v3_raw(
            content=prompt, max_tokens=max_tokens
        )
        return gpt_answer, stop_reason
    except Exception as e:
        print(f"Error calling GPT API: {e}")
        return "", ""

# def extract_python_code(response_text):
#     match = re.search(r"```python(.*?)```", response_text, re.DOTALL)
#     return match.group(1).strip() if match else ""

def process_row(index, client, item, max_tokens=2048):
    origin_img, mimic_img = item.get('origin_image', ""), item.get('mimic_image', "")
    if not origin_img or not mimic_img:
        print(f"No image path found for index {index}. Skipping.")
        return None
    
    prompt = construct_conversation(origin_img, mimic_img)
    gpt_answer, stop_reason = get_revised_text(client, prompt, max_tokens)
    
    result = {
        'gpt_grade': gpt_answer,
        'grade_stop_reason': stop_reason,
    }
    result.update(item)
    return result

def main():
    save_path = f"/mnt/lingjiejiang/multimodal_code/data/chart_data/ChartBench/chart46k_evol_47k_images_46k_code_dpo_94k_split_{MACHINE_ID}.json"
    # save_path = "/mnt/lingjiejiang/multimodal_code/data/dpo/chart/github_gpt4o_chart_47k_code_img.json"
    output_dir = f'/mnt/lingjiejiang/multimodal_code/data/dpo/chart/github_gpt4o_chart_94k_grade_{MACHINE_ID}'
    # output_dir = "tests/"
    os.makedirs(output_dir, exist_ok=True)
    
    data = load_data(save_path)
    # data = data[:2]
    clients = [Openai(apis=[API_INFOS[i]]) for i in range(len(API_INFOS))]

    print(f"save_path: {save_path}")
    print(f"output_dir: {output_dir}")
        
    max_tokens = 2048
    batch_size = 1000
    revised_data = []
    
    with ThreadPoolExecutor(max_workers=len(clients)) as executor:
        futures = [
            executor.submit(process_row, i, clients[i % len(clients)], item, max_tokens)
            for i, item in enumerate(data)
        ]
        
        for i, future in enumerate(tqdm(as_completed(futures), total=len(futures))):
            result = future.result()
            if result:
                revised_data.append(result)
            if (i + 1) % batch_size == 0:
                batch_number = (i + 1) // batch_size
                intermediate_file = os.path.join(output_dir, f"intermediate_batch_{batch_number}.json")
                with open(intermediate_file, 'w') as f:
                    json.dump(revised_data, f, indent=2)
    
    final_file = os.path.join(output_dir, "final_output.json")
    with open(final_file, 'w') as f:
        json.dump(revised_data, f, indent=2)

if __name__ == "__main__":
    main()
