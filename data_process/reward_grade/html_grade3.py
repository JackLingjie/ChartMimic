import json  
import os  
from concurrent.futures import ThreadPoolExecutor, as_completed  
from tqdm import tqdm  
from gpt4o import Openai, API_INFOS  
import re  
import base64

PROMPT_TEMPLATE = \
"""

You are an expert in evaluating the accuracy of AI-generated HTML visualizations. The first image (reference image) is created using ground truth html code, and the second image (AI-generated image) is created using html code generated by an AI assistant. 
Your task is to score the AI-generated image based on how well it replicates the reference image.

### Scoring Methodology:
The AI-generated image's score is based on the following criteria, totaling a score out of 100 points:

1. **Layout Consistency (30 points)**  
   - Does the overall structure and positioning of elements match the reference image?  
   - Are sections, divs, and key layout elements placed in the same relative positions?

2. **Element Alignment (20 points)**  
   - Are components such as images, buttons, and text boxes aligned in the same way as in the reference image?  
   - Are margins, paddings, and spacing between elements consistent?

3. **Color Scheme Accuracy (10 points)**  
   - Do background colors, text colors, and other visual styles match the reference image?  
   - Are gradients, shadows, and other stylistic elements faithfully reproduced?

4. **Proportional Accuracy (10 points)**  
   - Do images, buttons, and text boxes maintain the same proportions and aspect ratios?  
   - Are element sizes correctly scaled in relation to each other?

5. **Textual Content Match (20 points)**  
   - Is the text identical to the reference image in terms of content, font style, size, and formatting?  
   - Are there any missing, incorrect, or extra words?

### Evaluation:
Compare the two images head to head and provide a detailed assessment. Use the following format for your response:

---

Comments:
- Layout Consistency: ${your comment and subscore}
- Element Alignment: ${your comment and subscore}
- Color Scheme: ${your comment and subscore}
- Proportional Accuracy: ${your comment and subscore}
- Textual Content Match: ${your comment and subscore}

Score: ${your final score out of 100}

---

Please use the above format to ensure the evaluation is clear and comprehensive.
"""
  
MACHINE_ID = 3

def load_data(file_path):
    try:
        with open(file_path, 'r') as f:
            return json.load(f)
    except Exception as e:
        print(f"Error loading data from {file_path}: {e}")
        return []

def construct_conversation(original_png, generated_png):
    def encode_image(image_path):
        with open(image_path, "rb") as image_file:
            return base64.b64encode(image_file.read()).decode('utf-8')   
    base64_image1 = encode_image(original_png)      
    base64_image2 = encode_image(generated_png)
    system = "You are a helpful assistant."
    messages = [
        {"role": "system", "content": system},
        {"role": "user", "content": [
            {"type": "text", "text": f"{PROMPT_TEMPLATE}\n"},
            {"type": "text", "text": "<Ground Truth Image>"},
            {
                "image":base64_image1
            },
            {"type": "text", "text": "<AI-generated Image>"},
            {
                "image":base64_image2
            }
        ]
            },
    ]
    return messages

def get_revised_text(client, prompt, max_tokens=2048):
    try:
        gpt_answer, stop_reason = client.get_image_response_v3_raw(
            content=prompt, max_tokens=max_tokens
        )
        return gpt_answer, stop_reason
    except Exception as e:
        print(f"Error calling GPT API: {e}")
        return "", ""

# def extract_python_code(response_text):
#     match = re.search(r"```python(.*?)```", response_text, re.DOTALL)
#     return match.group(1).strip() if match else ""

def process_row(index, client, item, max_tokens=2048):
    origin_img, mimic_img = item.get('origin_image', ""), item.get('mimic_image', "")
    if not origin_img or not mimic_img:
        print(f"No image path found for index {index}. Skipping.")
        return None
    
    prompt = construct_conversation(origin_img, mimic_img)
    gpt_answer, stop_reason = get_revised_text(client, prompt, max_tokens)
    
    result = {
        'gpt_grade': gpt_answer,
        'grade_stop_reason': stop_reason,
    }
    result.update(item)
    return result

def main():
    save_path = f"/mnt/lingjiejiang/multimodal_code/data/dpo/html/gpt4o_html_dpo_92k_big_img_filtered_split{MACHINE_ID}.json"
    # save_path = "/mnt/lingjiejiang/multimodal_code/data/dpo/chart/github_gpt4o_chart_47k_code_img.json"
    output_dir = f'/mnt/lingjiejiang/multimodal_code/data/dpo/html/gpt4o_html_dpo_92k_big_img_filtered_grade_{MACHINE_ID}'
    # output_dir = "tests/"
    os.makedirs(output_dir, exist_ok=True)
    
    data = load_data(save_path)
    # data = data[:2]
    clients = [Openai(apis=[API_INFOS[i]]) for i in range(len(API_INFOS))]

    print(f"save_path: {save_path}")
    print(f"output_dir: {output_dir}")
        
    max_tokens = 2048
    batch_size = 1000
    revised_data = []
    
    with ThreadPoolExecutor(max_workers=len(clients)) as executor:
        futures = [
            executor.submit(process_row, i, clients[i % len(clients)], item, max_tokens)
            for i, item in enumerate(data)
        ]
        
        for i, future in enumerate(tqdm(as_completed(futures), total=len(futures))):
            result = future.result()
            if result:
                revised_data.append(result)
            if (i + 1) % batch_size == 0:
                batch_number = (i + 1) // batch_size
                intermediate_file = os.path.join(output_dir, f"intermediate_batch_{batch_number}.json")
                with open(intermediate_file, 'w') as f:
                    json.dump(revised_data, f, indent=2)
    
    final_file = os.path.join(output_dir, "final_output.json")
    with open(final_file, 'w') as f:
        json.dump(revised_data, f, indent=2)

if __name__ == "__main__":
    main()
